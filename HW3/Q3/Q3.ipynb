{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages & Set Up Data Layout\n",
    "\n",
    "## Preprocessing Information for the Given Data.\n",
    "\n",
    "A high-pass filter with a 30 Hz cut-off frequency and a power line notch filter (50 Hz) were used. All recordings are artifact-free EEG segments of 60 seconds duration. At the stage of data preprocessing, the Independent Component Analysis (ICA) was used to eliminate the artifacts (eyes, muscle, and cardiac overlapping of the cardiac pulsation). The arithmetic task was the serial subtraction of two numbers. Each trial started with the communication orally 4-digit (minuend) and 2-digit (subtrahend) numbers (e.g. 3141 and 42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject06_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 90999  =      0.000 ...   181.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject06_2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 30999  =      0.000 ...    61.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject07_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 90999  =      0.000 ...   181.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject07_2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 30999  =      0.000 ...    61.998 secs...\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "Data Shape Before: 21 91000\n",
      "Data Shape After: 21 303 300\n",
      "Data Shape Before: 21 31000\n",
      "Data Shape After: 21 103 300\n",
      "Data Shape Before: 21 91000\n",
      "Data Shape After: 21 303 300\n",
      "Data Shape Before: 21 31000\n",
      "Data Shape After: 21 103 300\n",
      "Shape of segmented data: (21, 812, 300)\n",
      "Shape of labels: (812,)\n"
     ]
    }
   ],
   "source": [
    "# Let's load some packages we need (pip install mne)\n",
    "import mne\n",
    "import mne.viz\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.channels import make_standard_montage\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "# ! pip install mne\n",
    "\n",
    "# Read raw data files where each file contains a run\n",
    "files = ['../../datasets/HW2Datasets/Subject06_1.edf', '../../datasets/HW2Datasets/Subject06_2.edf', '../../datasets/HW2Datasets/Subject07_1.edf', '../../datasets/HW2Datasets/Subject07_2.edf']\n",
    "\n",
    "# Read the raw EDF files into an array\n",
    "raws = [read_raw_edf(f, preload=True) for f in files]\n",
    "\n",
    "# Loop through the array and make the following changes to the raw files\n",
    "for raw in raws:\n",
    "\n",
    "    # Rename the raw channels\n",
    "    raw.rename_channels({'EEG F3':'F3', 'EEG F4':'F4',\n",
    "                            'EEG Fp1':'Fp1', 'EEG Fp2':'Fp2', 'EEG F7':'F7', 'EEG F8':'F8',\n",
    "                            'EEG T3':'T3', 'EEG T4':'T4', 'EEG C3':'C3', 'EEG C4':'C4',\n",
    "                            'EEG T5':'T5', 'EEG T6':'T6', 'EEG P3':'P3', 'EEG P4':'P4',\n",
    "                            'EEG O1':'O1', 'EEG O2':'O2', 'EEG Fz':'Fz', 'EEG Cz':'Cz',\n",
    "                            'EEG Pz':'Pz', 'EEG A2-A1':'A2', 'ECG ECG':'ECG'})\n",
    "\n",
    "\n",
    "    # Set channel types\n",
    "    raw.set_channel_types({'ECG':'ecg'})\n",
    "\n",
    "    # Define the channel locations\n",
    "    raw.set_montage(mne.channels.make_standard_montage('standard_1020'))\n",
    "\n",
    "    # Print Raw Channel Names for double checking\n",
    "    print(raw.ch_names)\n",
    "\n",
    "# Rename the raws with more insightfull names\n",
    "subject6_background = raws[0] # Subject 6 background raw\n",
    "subject6_task = raws[1] # Subject 6 task raw\n",
    "subject7_background = raws[2] # Subject 7 background raw\n",
    "subject7_task = raws[3] # Subject 7 task raw\n",
    "\n",
    "# Function to segment data into non-overlapping windows of length 300 samples\n",
    "def segment_data(raw, window_size=300):\n",
    "    data = raw.get_data()  # Get the raw data\n",
    "    n_channels, n_samples = data.shape # get dimensions\n",
    "    print(\"Data Shape Before:\", n_channels, n_samples) # display dimensions for understanding\n",
    "    n_windows = n_samples // window_size  # Number of windows\n",
    "\n",
    "    # Reshape data into (n_channels, n_windows, window_size)\n",
    "    segmented_data = data[:, :n_windows * window_size].reshape(n_channels, n_windows, window_size)\n",
    "    print(\"Data Shape After:\", n_channels, n_windows, window_size) # display shape after reshaping\n",
    "\n",
    "    return segmented_data # return the segmented data\n",
    "\n",
    "\n",
    "# Segment each raw file into windows\n",
    "subject6_background_segments = segment_data(subject6_background)\n",
    "subject6_task_segments = segment_data(subject6_task)\n",
    "subject7_background_segments = segment_data(subject7_background)\n",
    "subject7_task_segments = segment_data(subject7_task)\n",
    "\n",
    "# Create labels: 0 for background, 1 for task\n",
    "subject6_background_labels = np.zeros(subject6_background_segments.shape[1])\n",
    "subject6_task_labels = np.ones(subject6_task_segments.shape[1])\n",
    "subject7_background_labels = np.zeros(subject7_background_segments.shape[1])\n",
    "subject7_task_labels = np.ones(subject7_task_segments.shape[1])\n",
    "\n",
    "# Concatenate data for both subjects\n",
    "X = np.concatenate([subject6_background_segments, subject6_task_segments, \n",
    "                    subject7_background_segments, subject7_task_segments], axis=1)\n",
    "\n",
    "# Concatenate labels for both subjects\n",
    "y = np.concatenate([subject6_background_labels, subject6_task_labels,\n",
    "                    subject7_background_labels, subject7_task_labels])\n",
    "\n",
    "# X shape will be (n_channels, total_windows * window_size), and y will be the labels for each window\n",
    "print(\"Shape of segmented data:\", X.shape) # See the dimensions of X\n",
    "print(\"Shape of labels:\", y.shape) # See the dimensions of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3) \n",
    "## Apply three different feature engineering methods of your choice (feature extraction, transformation, or selection), and repeat the analysis in (Q2). Discuss how your results differ and how you could improve your results.\n",
    "\n",
    "TODO: Implement FFT (feature extraction) + Normalization/Scaling (transformation with z-score) + SMOTE (selection)\n",
    "And explain why you choose them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after FFT: (21, 812, 150)\n",
      "Examining fold 1\n",
      "Accuracy: 70.37%\n",
      "Balanced Accuracy: 70.40%\n",
      "F1 Score: 72.31%\n",
      "\n",
      "Examining fold 2\n",
      "Accuracy: 76.95%\n",
      "Balanced Accuracy: 77.00%\n",
      "F1 Score: 74.07%\n",
      "\n",
      "Examining fold 3\n",
      "Accuracy: 86.78%\n",
      "Balanced Accuracy: 86.78%\n",
      "F1 Score: 87.69%\n",
      "\n",
      "Examining fold 4\n",
      "Accuracy: 85.54%\n",
      "Balanced Accuracy: 85.54%\n",
      "F1 Score: 86.59%\n",
      "\n",
      "Examining fold 5\n",
      "Accuracy: 79.75%\n",
      "Balanced Accuracy: 79.75%\n",
      "F1 Score: 80.63%\n",
      "\n",
      "Average Accuracy Score: 79.88%\n",
      "Average Balanced Accuracy Score: 79.89%\n",
      "Average F1 Score: 80.26%\n"
     ]
    }
   ],
   "source": [
    "# Import ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "# Import library for k-folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Import library for fast fourier transform\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Import library for equal data distribution\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_fft(X, n_fft=300):\n",
    "    # X is of shape (n_samples, n_channels, n_points_per_window)\n",
    "    X_fft = np.abs(fft(X, n=n_fft, axis=2))  # FFT along the last axis (window axis)\n",
    "    return X_fft[:, :, :n_fft//2]  # Take only the positive frequencies (half of the spectrum)\n",
    "\n",
    "# Apply FFT to the data (shape will still be (n_samples, n_channels, n_features_per_channel))\n",
    "X_fft = apply_fft(X)\n",
    "print(\"Shape after FFT:\", X_fft.shape)\n",
    "\n",
    "# Reshape the data for model training (n_samples, n_features)\n",
    "X_reshaped = X_fft.reshape(X_fft.shape[1], -1)  # (n_windows, n_channels * window_size)\n",
    "# X_reshaped = X.reshape(X.shape[1], -1)  # (n_windows, n_channels * window_size)\n",
    "\n",
    "# Apply SMOTE after transformation\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_reshaped, y)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create k-folds where k=5\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "folds = skf.split(X_resampled, y_resampled) # make different folds for X and y\n",
    "\n",
    "train_idxs=[] # store training indexes\n",
    "test_idxs=[] # store test indexes\n",
    "\n",
    "# Loop through all folds\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # Grab indexes from fold\n",
    "    train_idxs.append(train_idx) # append training indexes to the training list\n",
    "    test_idxs.append(test_idx) # append testing indexes to the testing list\n",
    "\n",
    "accuracy_arr = []\n",
    "balanced_accuracy_arr = []\n",
    "f1_score_arr = []\n",
    "\n",
    "# Loop through the 5 folds made previously\n",
    "for i in range(5):\n",
    "    X_train = X_resampled[train_idxs[i][:]] # Load in the training X values from index i\n",
    "    y_train = y_resampled[train_idxs[i][:]] # Load in the training y values from index i\n",
    "    X_test = X_resampled[test_idxs[i][:]] # Load in the testing X values from index i\n",
    "    y_test = y_resampled[test_idxs[i][:]] # Load in the testing y values from index i\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print out the current fold we are itterating over\n",
    "    print(\"Examining fold %i\" % (i + 1))\n",
    "\n",
    "    # Evaluate the model using accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_arr.append(accuracy) # Append accuracy to array\n",
    "\n",
    "    # Evaluate the model using balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy_arr.append(balanced_accuracy) # Append balanced accuracy to array\n",
    "\n",
    "    # Evaluate the model using f1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_score_arr.append(f1) # Append f1 score to array\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Print balanced accuracy\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Print f1 score\n",
    "    print(f\"F1 Score: {f1*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"Average Accuracy Score: {np.sum(accuracy_arr)*100/5:.2f}%\")\n",
    "print(f\"Average Balanced Accuracy Score: {np.sum(balanced_accuracy_arr)*100/5:.2f}%\")\n",
    "print(f\"Average F1 Score: {np.sum(f1_score_arr)*100/5:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
