{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages & Set Up Data Layout\n",
    "\n",
    "## Preprocessing Information for the Given Data.\n",
    "\n",
    "A high-pass filter with a 30 Hz cut-off frequency and a power line notch filter (50 Hz) were used. All recordings are artifact-free EEG segments of 60 seconds duration. At the stage of data preprocessing, the Independent Component Analysis (ICA) was used to eliminate the artifacts (eyes, muscle, and cardiac overlapping of the cardiac pulsation). The arithmetic task was the serial subtraction of two numbers. Each trial started with the communication orally 4-digit (minuend) and 2-digit (subtrahend) numbers (e.g. 3141 and 42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject06_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 90999  =      0.000 ...   181.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject06_2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 30999  =      0.000 ...    61.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject07_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 90999  =      0.000 ...   181.998 secs...\n",
      "Extracting EDF parameters from /home/joshua/Desktop/MainFolder/OuClasses/2024 Fall/Neural-Data-Science/datasets/HW2Datasets/Subject07_2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 30999  =      0.000 ...    61.998 secs...\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'T3', 'T4', 'C3', 'C4', 'T5', 'T6', 'P3', 'P4', 'O1', 'O2', 'Fz', 'Cz', 'Pz', 'A2', 'ECG']\n",
      "Data Shape Before: 21 91000\n",
      "Data Shape After: 21 303 300\n",
      "Data Shape Before: 21 31000\n",
      "Data Shape After: 21 103 300\n",
      "Data Shape Before: 21 91000\n",
      "Data Shape After: 21 303 300\n",
      "Data Shape Before: 21 31000\n",
      "Data Shape After: 21 103 300\n",
      "Shape of segmented data: (812, 6300)\n",
      "Shape of labels: (812,)\n"
     ]
    }
   ],
   "source": [
    "# Let's load some packages we need (pip install mne)\n",
    "import mne\n",
    "import mne.viz\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.channels import make_standard_montage\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "# ! pip install mne\n",
    "\n",
    "# Read raw data files where each file contains a run\n",
    "files = ['../../datasets/HW2Datasets/Subject06_1.edf', '../../datasets/HW2Datasets/Subject06_2.edf', '../../datasets/HW2Datasets/Subject07_1.edf', '../../datasets/HW2Datasets/Subject07_2.edf']\n",
    "\n",
    "# Read the raw EDF files into an array\n",
    "raws = [read_raw_edf(f, preload=True) for f in files]\n",
    "\n",
    "# Loop through the array and make the following changes to the raw files\n",
    "for raw in raws:\n",
    "\n",
    "    # Rename the raw channels\n",
    "    raw.rename_channels({'EEG F3':'F3', 'EEG F4':'F4',\n",
    "                            'EEG Fp1':'Fp1', 'EEG Fp2':'Fp2', 'EEG F7':'F7', 'EEG F8':'F8',\n",
    "                            'EEG T3':'T3', 'EEG T4':'T4', 'EEG C3':'C3', 'EEG C4':'C4',\n",
    "                            'EEG T5':'T5', 'EEG T6':'T6', 'EEG P3':'P3', 'EEG P4':'P4',\n",
    "                            'EEG O1':'O1', 'EEG O2':'O2', 'EEG Fz':'Fz', 'EEG Cz':'Cz',\n",
    "                            'EEG Pz':'Pz', 'EEG A2-A1':'A2', 'ECG ECG':'ECG'})\n",
    "\n",
    "\n",
    "    # Set channel types\n",
    "    raw.set_channel_types({'ECG':'ecg'})\n",
    "\n",
    "    # Define the channel locations\n",
    "    raw.set_montage(mne.channels.make_standard_montage('standard_1020'))\n",
    "\n",
    "    # Print Raw Channel Names for double checking\n",
    "    print(raw.ch_names)\n",
    "\n",
    "# Rename the raws with more insightfull names\n",
    "subject6_background = raws[0] # Subject 6 background raw\n",
    "subject6_task = raws[1] # Subject 6 task raw\n",
    "subject7_background = raws[2] # Subject 7 background raw\n",
    "subject7_task = raws[3] # Subject 7 task raw\n",
    "\n",
    "# Function to segment data into non-overlapping windows of length 300 samples\n",
    "def segment_data(raw, window_size=300):\n",
    "    data = raw.get_data()  # Get the raw data\n",
    "    n_channels, n_samples = data.shape # get dimensions\n",
    "    print(\"Data Shape Before:\", n_channels, n_samples) # display dimensions for understanding\n",
    "    n_windows = n_samples // window_size  # Number of windows\n",
    "\n",
    "    # Reshape data into (n_channels, n_windows, window_size)\n",
    "    segmented_data = data[:, :n_windows * window_size].reshape(n_channels, n_windows, window_size)\n",
    "    print(\"Data Shape After:\", n_channels, n_windows, window_size) # display shape after reshaping\n",
    "\n",
    "    return segmented_data # return the segmented data\n",
    "\n",
    "\n",
    "# Segment each raw file into windows\n",
    "subject6_background_segments = segment_data(subject6_background)\n",
    "subject6_task_segments = segment_data(subject6_task)\n",
    "subject7_background_segments = segment_data(subject7_background)\n",
    "subject7_task_segments = segment_data(subject7_task)\n",
    "\n",
    "# Create labels: 0 for background, 1 for task\n",
    "subject6_background_labels = np.zeros(subject6_background_segments.shape[1])\n",
    "subject6_task_labels = np.ones(subject6_task_segments.shape[1])\n",
    "subject7_background_labels = np.zeros(subject7_background_segments.shape[1])\n",
    "subject7_task_labels = np.ones(subject7_task_segments.shape[1])\n",
    "\n",
    "# Concatenate data for both subjects\n",
    "X = np.concatenate([subject6_background_segments, subject6_task_segments, \n",
    "                    subject7_background_segments, subject7_task_segments], axis=1)\n",
    "\n",
    "# Concatenate labels for both subjects\n",
    "y = np.concatenate([subject6_background_labels, subject6_task_labels,\n",
    "                    subject7_background_labels, subject7_task_labels])\n",
    "\n",
    "# Reshape the data for model training (n_samples, n_features)\n",
    "X = X.reshape(X.shape[1], -1)  # (n_windows, n_channels * window_size)\n",
    "\n",
    "# X shape will be (n_channels, total_windows * window_size), and y will be the labels for each window\n",
    "print(\"Shape of segmented data:\", X.shape) # See the dimensions of X\n",
    "print(\"Shape of labels:\", y.shape) # See the dimensions of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2) \n",
    "## Repeat the analysis in (Q1) using the 5-fold cross validation technique. Compare the results with the previous case and provide detailed explanation of potential causes of these differences.\n",
    "\n",
    "Previously..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Train Values 406\n",
      "Total Number of y=1 Train Values 100.0\n",
      "Accuracy: 73.89%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Import ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Split the dataset into 50% train and 50% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "print(\"Total Number of Train Values\", len(y_train)) # See number of trained values\n",
    "print(\"Total Number of y=1 Train Values\", sum(y_train)) # See number of y=1 values\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model using balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model using f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Print balanced accuracy\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy*100:.2f}%\")\n",
    "\n",
    "# Print f1 score\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Question (Q1) I talked in detail why this model sucks & explained what each of the evaluation metrics mean.\n",
    "\n",
    "This time we will do the same procedure as above, but using k-folds with k=5. This will allow us to have 5 different training and evaluating pools which could lead to different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining fold 1\n",
      "Accuracy: 74.85%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n",
      "\n",
      "Examining fold 2\n",
      "Accuracy: 74.23%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n",
      "\n",
      "Examining fold 3\n",
      "Accuracy: 74.69%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n",
      "\n",
      "Examining fold 4\n",
      "Accuracy: 74.69%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n",
      "\n",
      "Examining fold 5\n",
      "Accuracy: 74.69%\n",
      "Balanced Accuracy: 50.00%\n",
      "F1 Score: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import library for k-folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create k-folds where k=5\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "folds = skf.split(X, y) # make different folds for X and y\n",
    "\n",
    "train_idxs=[] # store training indexes\n",
    "test_idxs=[] # store test indexes\n",
    "\n",
    "# Loop through all folds\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # Grab indexes from fold\n",
    "    train_idxs.append(train_idx) # append training indexes to the training list\n",
    "    test_idxs.append(test_idx) # append testing indexes to the testing list\n",
    "\n",
    "# Loop through the 5 folds made previously\n",
    "for i in range(5):\n",
    "    X_train = X[train_idxs[i][:]] # Load in the training X values from index i\n",
    "    y_train = y[train_idxs[i][:]] # Load in the training y values from index i\n",
    "    X_test = X[test_idxs[i][:]] # Load in the testing X values from index i\n",
    "    y_test = y[test_idxs[i][:]] # Load in the testing y values from index i\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print out the current fold we are itterating over\n",
    "    print(\"Examining fold %i\" % (i + 1))\n",
    "\n",
    "    # Evaluate the model using accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Evaluate the model using balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Evaluate the model using f1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Print balanced accuracy\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Print f1 score\n",
    "    print(f\"F1 Score: {f1*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the F1 Score is still 0% and the Balanced Accuracy is still 50% which tells us that our model given different training data in terms of being divided into k-folds is not statisfactory enough change in order to make the model work as intended. That intended thing being that the model can accurately predict if brain signals are associated with a task as well as a background, not just background which the model is doing here just as it was doing before.\n",
    "\n",
    "Also the Accuracy metric is changing throughout the folds sometimes and the reason for that is because the accuracy metric is more sensitive to little changes in the data (one more y=0 improves score) in comparision to the other metrics in this circumstance (in F1 Score & Balanced Accuracy, more y=0 will never improve this score because the other term is 0, removing the chances for any improvement).\n",
    "\n",
    "Overall these metric results given with k-folds are almost the same as the results given in Q1. F1 Score is the same, Balanced Accuracy is the same, and Accuracy is 1% different given the fold itteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
